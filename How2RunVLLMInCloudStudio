vLLM运行Qwen3-30B-A3B-Thinking-2507-AWQ-4bit量化模型，提供API Web接口给CherryStudio使用
1）20C116G内存24G显存A10的GPU速度最大可达200t/s，6并发吞吐量达到峰值300~400t/s（并发数>6后就开始排队等待），GPU用到97%显存用到21.7/22.5但CPU和内存基本都是空闲，50GB硬盘用了53GB但重启后
   还是可以直接用的（应该是模型文件+vLLM23GB没有超50GB，加上一些临时文件超了），该环境下的极限值最大上下文长度16384和GPU显存利用率0.9
2）8C140G内存32G显存V100的GPU 由于GPU太老计算能力 7.0，跑不了AWQ模型（计算能力至少为 8.0SM80），所以V100机器建议用GPTQ的量化模型

==
安装方法：
应用空间选Ubuntu + Docker安装vLLM
API接口：https://2d255b6bdde54e2996aa98333d5bc10d--8000.ap-shanghai2.cloudstudio.club/v1/
======完整安装过程如下====
export HF_HOME=~/.cache/huggingface
mkdir -p "$HF_HOME"
docker pull vllm/vllm-openai:latest 

### 重启后用下面命令启动模型！api-key用多个sk-key1,sk-key2测试失败只能一个key！24GA10显卡用这个AWQ量化模型
export HF_HOME=~/.cache/huggingface
docker run --rm --gpus all --ipc=host \
  -p 8000:8000 \
  -v "$HF_HOME":/root/.cache/huggingface \
  vllm/vllm-openai:latest \
  --model cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit \
  --api-key sk-123 \
  --dtype auto \
  --max-model-len 16384 \
  --gpu-memory-utilization 0.90
== 重启后用下面命令启动模型！api-key用多个sk-key1,sk-key2测试失败只能一个key！32GV100显卡用这个，但目前还没有启动成功过？？？
export HF_HOME=~/.cache/huggingface
export TORCHDYNAMO_DISABLE=1
export VLLM_DISABLE_FA2=true

docker run --rm --gpus all --ipc=host \
  -p 8000:8000 \
  -v "$HF_HOME":/root/.cache/huggingface \
  vllm/vllm-openai:latest \
  --model btbtyler09/Qwen3-30B-A3B-Thinking-2507-gptq-4bit \
  --api-key sk-123 \
  --dtype auto \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.85 \
  --reasoning-parser qwen3
====
如果失败报类似下面的错误（本质是nVidia在docker中运行错，要打开一些权限）见https://github.com/bccwuho/How2RunOllamaInCloudStudio 1.2的解决方法
docker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running prestart hook #0: exit status 1, stdout: , stderr: Auto-detected mode as 'legacy'
nvidia-container-cli: mount error: failed to add device rules: unable to find any existing device filters attached to the cgroup: bpf_prog_query(BPF_CGROUP_DEVICE) failed: operation not permitted: unknown.

====
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit",
    "messages": [{"role":"user","content":"用中文一步步思考：24 * 17 等于多少？"}],
    "stream": false
  }'
