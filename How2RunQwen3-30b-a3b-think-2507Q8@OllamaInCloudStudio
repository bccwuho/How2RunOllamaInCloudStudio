# How to Run Qwen3-30b-a3b-think-2507Q8@Ollama In CloudStudio

最终结论：
1）🔴在24GB显存A10机器上不足容纳qwen3-30b-a3b2507Q8（完成诸如翻译/辅导高中生作业/中级程序员之类的工作（智力~GPT4.35接近DS R1满血老版本））的32GB模型需要GPU/CPU混合推理的情况下，速度只有Prefill是主要瓶颈（KV设q8_0时实测推理生成速度为30t/s，Prefill速度受混合推理拖累实测40t/s）
2）🔴业务上Ollama只能为单人服务且在用Cherry Studio客户端时体验受限但也能具备在一定上下文条件下，在Ollama CLI上体验超好，实测过使用2hour以上完全正常
具体体验受限如下：
1）在Cherry Studio中首次最大问题输入控制在12K，上下文可以达到32K最大输出20K时保持在一个对话中（防止KV cache切换对话时clear）持续问完体验稳定（长对话时保持耐心且不要中断，例如几何题-5全对，回答一般在12-20K需要大致15-30min）；当Prefill 5min超时或被中断时系统对后面的对话无反应，此时restart docker后重新加载模型即可
2）在ollama CLI下没有上下文的限制（哪怕模型文件中设了num_ctx=32K）也不会Prefill Timeout（似乎KV Cache一直可以扩展只要内存够），实际对话超2hours，我的测试题目走了近2遍，一切都还正常，体验非常好除了MD格式没有render外
3）32G的V100 Q8和Q4实测速度都只有A10机器的80%，分别是Q4 80t/s，Q8 24t/s（虽绝大部分都在GPU上了但V100机器CPU是8C比A10的20C弱不少），且比A10还贵，所以一般不用！
4）在22.5G显存的A10的Ollama下运行qwen3-30b-a3b-2507-Q8模型最佳参数：
1、OLLAMA_GPU_OVERHEAD 为1GB
2、OLLAMA_KV_CACHE_TYPE=q8_0  ；减小KV cache占用VRAM，加速Prefill速度（实测5minTimeout时间够Prefill最大上文可以12-13K）
3、启用OLLAMA_FLASH_ATTENTION（可能是默认）
4、禁用mmap
5、Optional：PARAMETER num_ctx 32768         # 32K 上下文 和 PARAMETER num_predict 15360     # 最多一次性输出 ~15K？？？
6、这样在目标32K上下文问题（最大输出15K的）,KV cache不考虑flashAttention（可能是默认）和默认KV=fp16的情况下最大预留是3GB（部分在1.9G在显存1.1G在内存，约等于模型权重在VRAM和RAM上的比例）；KV=q8_0时最大预留是3GB，1G在显存，0.5G在内存
**现在本地跑LLM，最重要的是显存够加载完整的权重文件+预留KV Cache（32Kctx默认F16对于Qwen3-30b-a3b~3GB）；其次才是GPU，主流4070算力足够但16G显存不够，所以想用得好还要上24G4090或32G5090，成本依旧高；32G-128G大内存AMD8745（核显780接近2060）/AI-MAX 390（核显接近4060）可能是性价比高一点的替代；所以从业务上跑Qwen3-30b-a3b-AWQ4模型vLLM（擅长纯GPU）最合适6并发吞吐量能达200-300t/s，硬要上Qwen3-30b-a3b-Q8的话只能勉强Ollama一个人玩且上下文32KAI-MAX390可能达到40t/s？？？最佳方案可能是15K元的128GAI-MAX390跑Qwen3-30b-a3b-FP16@vLLM单人达到40t/s？？？**
