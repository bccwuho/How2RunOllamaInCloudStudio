# ç»“è®º
- **vLLMä½¿ç”¨çš„ä¸€äº›ä¼˜åŒ–æ‰‹æ®µéœ€è¦Aç³»åˆ—çš„GPUï¼Œæ‰€ä»¥A10å’ŒL40å¯ä»¥ï¼ŒV100å’ŒT4å·²ç»ä¸èƒ½æ”¯æŒäº†;è€ŒOllamaä¸Šå³ä½¿æ— GPUä¹Ÿèƒ½ç”¨**
- **Qwen3-30B-A3B-Thinking-2507-AWQ-4bitåœ¨Ollama@A10ä¸Šçš„é€Ÿåº¦èƒ½åˆ°100t/sã€å•å¹¶å‘ä¸”APIæ— æ³•é…keyï¼›vLLMé€Ÿåº¦èƒ½åˆ°200t/sã€ä¸æ’é˜Ÿèƒ½6å¹¶å‘ååé‡è¾¾åˆ°300-400t/sä¸”APIèƒ½é…å•ä¸ªkeyäº†ï¼Œç›¸å½“ä¼˜ç§€ï¼Œä½†24Gæ˜¾å­˜çš„A10åªèƒ½è·‘16Kä¸Šä¸‹æ–‡**
- ğŸ”´**Qwen3-30B-A3B-Thinking-2507-FP8å®æµ‹åœ¨vLLM@48C196Gå†…å­˜48Gæ˜¾å­˜L40ä¸Šå•å‘é€Ÿåº¦èƒ½åˆ°100t/sã€ä¸æ’é˜Ÿèƒ½6å¹¶å‘ååé‡è¾¾åˆ°240/sä¸”APIèƒ½é…å•ä¸ªkeyäº†ï¼Œä¸”ä¸Šä¸‹æ–‡èƒ½åˆ°100Kï¼Œæ€§èƒ½ç›¸å½“ä¼˜ç§€!!!**
- vLLMä¸Šç›®å‰CloudStudioèƒ½è·‘çš„æœ€å¥½æ¨¡å‹æ˜¯æ™ºåŠ›4.35çš„Qwen3-30B-A3B-Thinking-2507-FP8 å’Œ æ™ºåŠ›4.3çš„Qwen3-30B-A3B-Thinking-2507-AWQ-4bit

## 0.è…¾è®¯äº‘CloudStudioçš„CUDAé©±åŠ¨ å’Œ Docker å’Œ NVIDIA Container Toolkitéƒ½å·²ç»è£…å¥½ï¼Œä½†å¦‚æœé‡åˆ°ç±»ä¼¼ä¸‹é¢cgroupé—®é¢˜ï¼Œä¾‹å¦‚å¤±è´¥æŠ¥ç±»ä¼¼ä¸‹é¢çš„é”™è¯¯ï¼ˆæœ¬è´¨æ˜¯nVidiaåœ¨dockerä¸­è¿è¡Œé”™ï¼Œè¦æ‰“å¼€ä¸€äº›æƒé™ï¼‰æŒ‰ä¸€ä¸‹æ–¹æ³•è§£å†³å³å¯
docker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running prestart hook #0: exit status 1, stdout: , stderr: Auto-detected mode as 'legacy' <BR>
nvidia-container-cli: mount error: failed to add device rules: unable to find any existing device filters attached to the cgroup: bpf_prog_query(BPF_CGROUP_DEVICE) failed: operation not  <BR>permitted: unknown. <BR>

```bash
docker run --rm --gpus all nvidia/cuda:12.4.1-base-ubuntu22.04 nvidia-smi
```
**è‹¥èƒ½æ­£å¸¸æ˜¾ç¤º A10æˆ–L40 ä¸CUDAé©±åŠ¨ç‰ˆæœ¬ï¼Œå³è¡¨ç¤ºå®¹å™¨å¯ç”¨ GPU** <BR>

```bash
sudo mkdir -p /etc/nvidia-container-runtime
sudo nano /etc/nvidia-container-runtime/config.toml

Add or ensure the following 2 lines are present:

[nvidia-container-cli]
no-cgroups = true
###The key setting is no-cgroups = true, which disables cgroup device rule enforcement and avoids the bpf_prog_query call .

[nvidia-container-runtime]
debug = "/tmp/nvidia-container-runtime.log"

ç”¨^xï¼ˆæŒ‰CTRL+Xï¼‰é€€å‡ºç¼–è¾‘åå†æ¬¡è¿è¡Œollamaå¯åŠ¨å‘½ä»¤å°±OKäº†
```

## 1. vLLMè¿è¡ŒQwen3-30B-A3B-Thinking-2507-AWQ-4bité‡åŒ–æ¨¡å‹ï¼Œæä¾›API Webæ¥å£ç»™CherryStudioä½¿ç”¨ <BR>
1ï¼‰20C116Gå†…å­˜24Gæ˜¾å­˜A10çš„GPUé€Ÿåº¦æœ€å¤§å¯è¾¾200t/sï¼Œ6å¹¶å‘ååé‡è¾¾åˆ°å³°å€¼300~400t/sï¼ˆå¹¶å‘æ•°>6åå°±å¼€å§‹æ’é˜Ÿç­‰å¾…ï¼‰ï¼ŒGPUç”¨åˆ°97%æ˜¾å­˜ç”¨åˆ°21.7/22.5ä½†CPUå’Œå†…å­˜åŸºæœ¬éƒ½æ˜¯ç©ºé—²ï¼Œ50GBç¡¬ç›˜ç”¨äº†53GBä½†é‡å¯å 
   è¿˜æ˜¯å¯ä»¥ç›´æ¥ç”¨çš„ï¼ˆåº”è¯¥æ˜¯æ¨¡å‹æ–‡ä»¶+vLLM23GBæ²¡æœ‰è¶…50GBï¼ŒåŠ ä¸Šä¸€äº›ä¸´æ—¶æ–‡ä»¶è¶…äº†ï¼‰ï¼Œè¯¥ç¯å¢ƒä¸‹çš„æé™å€¼æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦16384å’ŒGPUæ˜¾å­˜åˆ©ç”¨ç‡0.9  <BR>
2ï¼‰8C140Gå†…å­˜32Gæ˜¾å­˜V100çš„GPU ç”±äºGPUå¤ªè€è®¡ç®—èƒ½åŠ› 7.0ï¼Œè·‘ä¸äº†AWQæ¨¡å‹ï¼ˆè®¡ç®—èƒ½åŠ›è‡³å°‘ä¸º 8.0SM80ï¼‰ï¼Œæ‰€ä»¥V100æœºå™¨å»ºè®®ç”¨GPTQçš„é‡åŒ–æ¨¡å‹ <BR>

### 1.1 å®‰è£…æ–¹æ³•ï¼š
åº”ç”¨ç©ºé—´é€‰Ubuntu + Dockerå®‰è£…vLLMhttps://github.com/bccwuho/How2RunOllamaInCloudStudio/blob/main/How2RunVLLMInCloudStudio.md  <BR>
APIæ¥å£ï¼šhttps://2d255b6bdde54e2996aa98333d5bc10d--8000.ap-shanghai2.cloudstudio.club/v1/   <BR>
======å®Œæ•´å®‰è£…è¿‡ç¨‹å¦‚ä¸‹====<BR>
```bash
export HF_HOME=~/.cache/huggingface   
mkdir -p "$HF_HOME"                  
docker pull vllm/vllm-openai:latest    
```

### 1.2 é‡å¯åç”¨ä¸‹é¢å‘½ä»¤å¯åŠ¨æ¨¡å‹ï¼api-keyç”¨å¤šä¸ªsk-key1,sk-key2æµ‹è¯•å¤±è´¥åªèƒ½ä¸€ä¸ªkeyï¼24GA10æ˜¾å¡ç”¨è¿™ä¸ªAWQé‡åŒ–æ¨¡å‹
```bash
export HF_HOME=~/.cache/huggingface                  
docker run --rm --gpus all --ipc=host \               
  -p 8000:8000 \                                    
  -v "$HF_HOME":/root/.cache/huggingface \         
  vllm/vllm-openai:latest \                       
  --model cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit \      
  --api-key sk-123 \                                 
  --dtype auto \                                    
  --max-model-len 16384 \                           
  --gpu-memory-utilization 0.90
```
    
== é‡å¯åç”¨ä¸‹é¢å‘½ä»¤å¯åŠ¨æ¨¡å‹ï¼api-keyç”¨å¤šä¸ªsk-key1,sk-key2æµ‹è¯•å¤±è´¥åªèƒ½ä¸€ä¸ªkeyï¼32GV100æ˜¾å¡ç”¨è¿™ä¸ªï¼Œä½†ç›®å‰è¿˜æ²¡æœ‰å¯åŠ¨æˆåŠŸè¿‡ï¼Ÿï¼Ÿï¼Ÿ   <BR>
```bash
export HF_HOME=~/.cache/huggingface               
export TORCHDYNAMO_DISABLE=1                     
export VLLM_DISABLE_FA2=true                     

docker run --rm --gpus all --ipc=host \          
  -p 8000:8000 \                                  
  -v "$HF_HOME":/root/.cache/huggingface \          
  vllm/vllm-openai:latest \                        
  --model btbtyler09/Qwen3-30B-A3B-Thinking-2507-gptq-4bit \       
  --api-key sk-123 \                                  
  --dtype auto \                                        
  --max-model-len 8192 \                                  
  --gpu-memory-utilization 0.85 \                         
  --reasoning-parser qwen3                            <
```

### 1.3 æµ‹è¯•
```bash
curl http://localhost:8000/v1/chat/completions \       
  -H "Content-Type: application/json" \                
  -d '{                                                 
    "model": "cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit",          
    "messages": [{"role":"user","content":"ç”¨ä¸­æ–‡ä¸€æ­¥æ­¥æ€è€ƒï¼š24 * 17 ç­‰äºå¤šå°‘ï¼Ÿ"}],       
    "stream": false                                     
  }'    <BR>
```

## 2. Qwen3-30B-A3B-Thinking-2507-FP8è¿è¡Œåœ¨vLLM@48C196Gå†…å­˜48Gæ˜¾å­˜ï¼ˆå®é™…45Gï¼‰L40
### 2.1 ä¸€ä¸ªå‘½ä»¤å®‰è£…vLLM + åŠ è½½Qwen3-30B-A3B-Thinking-2507-FP8æ¨¡å‹ï¼ˆæ¨¡å‹31GBæœ€å¤§ä¸Šä¸‹æ–‡100Kï¼ŒKV Cacheå¤§è‡´11GæŠŠ45Gæ˜¾å­˜åŸºæœ¬ç”¨è¶³äº†ï¼ï¼‰ å’Œ å¯åŠ¨APIæœåŠ¡
**çº¯ä¿ç•™å®¹å™¨ï¼ˆæœ€çœäº‹ï¼›ä¸æ˜ å°„å·ï¼‰, é‡å¯åä¸ä¼šé‡ä¸‹ï¼Œä½†å¦‚æœä»¥åä½  docker rm äº†å®¹å™¨ï¼Œç¼“å­˜å°±è·Ÿç€æ²¡äº†ã€‚**
```bash
docker run --name vllm-qwen -d --gpus all \
  --ipc=host \
  -p 8000:8000 \
  docker.io/vllm/vllm-openai:latest \
  --model Qwen/Qwen3-30B-A3B-Thinking-2507-FP8 \
  --gpu-memory-utilization 0.95 \
  --kv-cache-dtype fp8 \
  --max-model-len 100000 \
  --api-key sk-123
```

**ä¹‹åç”¨ä¸‹é¢å‘½ä»¤**
docker stop vllm-qwen               # åœæ­¢
docker start vllm-qwen              # åå°å¯åŠ¨
docker logs -f vllm-qwen            # çœ‹æ—¥å¿—
**æƒ³è¦å‰å°çœ‹æ—¥å¿—å¯åŠ¨ï¼šdocker start -ai vllm-qwen**


### 2.2 æœ¬åœ°æµ‹è¯•
```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-123" \
  -d '{
        "model": "Qwen/Qwen3-30B-A3B-Thinking-2507-FP8",   \
        "messages": [{"role":"user","content":"ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯è‡ªæˆ‘ä»‹ç»"}]   \
      }'
```

